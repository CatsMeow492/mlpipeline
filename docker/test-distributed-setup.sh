#!/bin/bash
# Test script for distributed computing Docker setup

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}Testing ML Pipeline Distributed Computing Setup${NC}"
echo "=================================================="

# Test 1: Build distributed images
echo -e "\n${YELLOW}Test 1: Building distributed computing images${NC}"
echo "Building production image with distributed support..."
docker build --target production -t mlpipeline:test-distributed . --quiet

echo "Building GPU production image with distributed support..."
docker build --target production-gpu -t mlpipeline:test-distributed-gpu . --quiet

echo -e "${GREEN}âœ“ Distributed computing images built successfully${NC}"

# Test 2: Validate distributed docker-compose configurations
echo -e "\n${YELLOW}Test 2: Validating distributed docker-compose configurations${NC}"
docker-compose -f docker-compose.yml --profile distributed config --quiet
echo "Main docker-compose with distributed profile validated"

docker-compose -f docker-compose.distributed.yml config --quiet
echo "Distributed docker-compose validated"

echo -e "${GREEN}âœ“ All docker-compose configurations are valid${NC}"

# Test 3: Test Dask scheduler startup
echo -e "\n${YELLOW}Test 3: Testing Dask scheduler startup${NC}"
DASK_SCHEDULER_ID=$(docker run -d --name test-dask-scheduler -p 8786:8786 -p 8787:8787 daskdev/dask:latest dask-scheduler --host 0.0.0.0)
sleep 10

# Check if Dask scheduler is running
if docker ps | grep -q test-dask-scheduler; then
    echo -e "${GREEN}âœ“ Dask scheduler started successfully${NC}"
    
    # Test dashboard accessibility
    if docker exec $DASK_SCHEDULER_ID python -c "import requests; requests.get('http://localhost:8787/status')" > /dev/null 2>&1; then
        echo -e "${GREEN}âœ“ Dask dashboard is accessible${NC}"
    else
        echo -e "${YELLOW}âš  Dask dashboard not accessible (may need more time)${NC}"
    fi
else
    echo -e "${RED}âœ— Dask scheduler failed to start${NC}"
    docker logs $DASK_SCHEDULER_ID
    exit 1
fi

# Cleanup Dask scheduler
docker stop $DASK_SCHEDULER_ID > /dev/null
docker rm $DASK_SCHEDULER_ID > /dev/null

# Test 4: Test Ray head node startup
echo -e "\n${YELLOW}Test 4: Testing Ray head node startup${NC}"
RAY_HEAD_ID=$(docker run -d --name test-ray-head -p 8265:8265 -p 10001:10001 rayproject/ray:latest ray start --head --dashboard-host=0.0.0.0 --dashboard-port=8265 --disable-usage-stats --block 2>/dev/null || echo "failed")

if [ "$RAY_HEAD_ID" != "failed" ]; then
    sleep 15
    
    # Check if Ray head is running
    if docker ps --format "table {{.Names}}" | grep -q test-ray-head 2>/dev/null; then
        echo -e "${GREEN}âœ“ Ray head node started successfully${NC}"
        
        # Test Ray connection (basic test)
        if docker exec $RAY_HEAD_ID python -c "import ray; print('Ray imported')" > /dev/null 2>&1; then
            echo -e "${GREEN}âœ“ Ray is accessible in container${NC}"
        else
            echo -e "${YELLOW}âš  Ray not accessible (may need more time)${NC}"
        fi
    else
        echo -e "${YELLOW}âš  Ray head node may not have started properly${NC}"
    fi
    
    # Cleanup Ray head
    docker stop $RAY_HEAD_ID > /dev/null 2>&1 || true
    docker rm $RAY_HEAD_ID > /dev/null 2>&1 || true
else
    echo -e "${YELLOW}âš  Ray head node startup test skipped due to Docker API issues${NC}"
fi

# Test 5: Test distributed computing imports in container
echo -e "\n${YELLOW}Test 5: Testing distributed computing imports${NC}"
echo "Testing Dask imports..."
if docker run --rm mlpipeline:test-distributed python -c "import dask; import dask.distributed; print('Dask imported successfully')" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ Dask imports successfully${NC}"
else
    echo -e "${RED}âœ— Dask import failed${NC}"
    exit 1
fi

echo "Testing Ray imports..."
if docker run --rm mlpipeline:test-distributed python -c "import ray; print('Ray imported successfully')" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ Ray imports successfully${NC}"
else
    echo -e "${RED}âœ— Ray import failed${NC}"
    exit 1
fi

echo "Testing ML Pipeline distributed modules..."
if docker run --rm mlpipeline:test-distributed python -c "from mlpipeline.distributed import DaskBackend, RayBackend, ResourceManager, DistributedScheduler; print('Distributed modules imported successfully')" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ ML Pipeline distributed modules import successfully${NC}"
else
    echo -e "${RED}âœ— ML Pipeline distributed modules import failed${NC}"
    exit 1
fi

# Test 6: Test resource monitoring
echo -e "\n${YELLOW}Test 6: Testing resource monitoring${NC}"
if docker run --rm mlpipeline:test-distributed python -c "
from mlpipeline.distributed.resource_manager import ResourceManager
rm = ResourceManager()
usage = rm.get_system_resources()
print(f'CPU: {usage.cpu_percent}%, Memory: {usage.memory_percent}%')
" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ Resource monitoring works${NC}"
else
    echo -e "${RED}âœ— Resource monitoring failed${NC}"
    exit 1
fi

# Test 7: Test distributed backends initialization (without actual clusters)
echo -e "\n${YELLOW}Test 7: Testing distributed backends initialization${NC}"
if docker run --rm mlpipeline:test-distributed python -c "
from mlpipeline.distributed.dask_backend import DaskBackend
from mlpipeline.distributed.ray_backend import RayBackend

# Test backend creation (without initialization)
dask_backend = DaskBackend()
ray_backend = RayBackend()
print('Backends created successfully')
" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ Distributed backends can be created${NC}"
else
    echo -e "${RED}âœ— Distributed backends creation failed${NC}"
    exit 1
fi

# Test 8: Test scheduler creation
echo -e "\n${YELLOW}Test 8: Testing distributed scheduler${NC}"
if docker run --rm mlpipeline:test-distributed python -c "
from mlpipeline.distributed.scheduler import DistributedScheduler
from mlpipeline.distributed.resource_manager import ResourceManager

rm = ResourceManager()
scheduler = DistributedScheduler(rm)
print('Scheduler created successfully')
" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ Distributed scheduler can be created${NC}"
else
    echo -e "${RED}âœ— Distributed scheduler creation failed${NC}"
    exit 1
fi

# Test 9: Test GPU container (if NVIDIA runtime available)
echo -e "\n${YELLOW}Test 9: Testing GPU distributed container${NC}"
if docker info | grep -q nvidia; then
    echo "NVIDIA Docker runtime detected, testing GPU distributed container..."
    GPU_CONTAINER_ID=$(docker run -d mlpipeline:test-distributed-gpu sleep 30)
    sleep 2
    if docker ps | grep -q $GPU_CONTAINER_ID; then
        echo -e "${GREEN}âœ“ GPU distributed container started successfully${NC}"
        
        # Test GPU libraries import
        if docker exec $GPU_CONTAINER_ID python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')" > /dev/null 2>&1; then
            echo -e "${GREEN}âœ“ GPU libraries accessible in distributed container${NC}"
        else
            echo -e "${YELLOW}âš  GPU libraries not accessible (expected without GPU)${NC}"
        fi
        
        docker stop $GPU_CONTAINER_ID > /dev/null
        docker rm $GPU_CONTAINER_ID > /dev/null
    else
        echo -e "${RED}âœ— GPU distributed container failed to start${NC}"
        exit 1
    fi
else
    echo -e "${YELLOW}âš  NVIDIA Docker runtime not available, skipping GPU test${NC}"
fi

# Test 10: Test complete distributed stack startup (quick test)
echo -e "\n${YELLOW}Test 10: Testing complete distributed stack startup${NC}"
echo "Starting minimal distributed stack..."

# Create a temporary docker-compose file for testing
cat > /tmp/test-distributed-compose.yml << EOF
version: '3.8'
services:
  dask-scheduler:
    image: daskdev/dask:latest
    command: ["dask-scheduler", "--host", "0.0.0.0"]
    ports:
      - "8786:8786"
      - "8787:8787"
  
  ray-head:
    image: rayproject/ray:latest
    command: ["ray", "start", "--head", "--dashboard-host=0.0.0.0", "--dashboard-port=8265", "--disable-usage-stats", "--block"]
    ports:
      - "8265:8265"
      - "10001:10001"

networks:
  default:
    driver: bridge
EOF

# Start the test stack
docker-compose -f /tmp/test-distributed-compose.yml up -d > /dev/null 2>&1
sleep 15

# Check if services are running
DASK_RUNNING=$(docker-compose -f /tmp/test-distributed-compose.yml ps -q dask-scheduler | wc -l)
RAY_RUNNING=$(docker-compose -f /tmp/test-distributed-compose.yml ps -q ray-head | wc -l)

if [ "$DASK_RUNNING" -eq 1 ] && [ "$RAY_RUNNING" -eq 1 ]; then
    echo -e "${GREEN}âœ“ Distributed stack started successfully${NC}"
else
    echo -e "${RED}âœ— Distributed stack failed to start completely${NC}"
    docker-compose -f /tmp/test-distributed-compose.yml logs
fi

# Cleanup test stack
docker-compose -f /tmp/test-distributed-compose.yml down > /dev/null 2>&1
rm /tmp/test-distributed-compose.yml

# Cleanup test images
echo -e "\n${YELLOW}Cleaning up test images${NC}"
docker rmi mlpipeline:test-distributed mlpipeline:test-distributed-gpu > /dev/null 2>&1 || true

echo -e "\n${GREEN}ðŸŽ‰ All distributed computing tests passed successfully!${NC}"
echo -e "${BLUE}Distributed computing setup is ready for deployment${NC}"
echo -e "\n${YELLOW}Available dashboards when running:${NC}"
echo -e "  Dask Dashboard: http://localhost:8787"
echo -e "  Ray Dashboard: http://localhost:8265"
echo -e "\n${YELLOW}Quick start commands:${NC}"
echo -e "  make up-distributed      # Start basic distributed services"
echo -e "  make up-distributed-full # Start full distributed stack"
echo -e "  make up-distributed-gpu  # Start GPU-enabled distributed stack"